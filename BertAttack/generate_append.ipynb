{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bertmodel\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "\n",
    "from utils_squad import read_squad_examples, convert_examples_to_features, RawResult, write_predictions\n",
    "from utils_squad_evaluate import EVAL_OPTS, main as evaluate_on_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(input_file, tokenizer):\n",
    "    cached_features_file = 'Ecached_dev_{}_{}'.format(model_name, str(max_seq_length))\n",
    "    \"\"\" \n",
    "    if os.path.exists(cached_features_file):\n",
    "        #print(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    \"\"\"\n",
    "    print(\"Creating features from dataset file at %s\", input_file)\n",
    "    examples = read_squad_examples(input_file=input_file,\n",
    "                                   is_training=False,\n",
    "                                   version_2_with_negative=False)\n",
    "    features = convert_examples_to_features(examples=examples,\n",
    "                                            tokenizer=tokenizer,\n",
    "                                            max_seq_length=max_seq_length,\n",
    "                                            doc_stride=doc_stride,\n",
    "                                            max_query_length=max_query_length,\n",
    "                                            is_training=False)\n",
    "    print(\"Saving features into cached file %s\", cached_features_file)\n",
    "    torch.save(features, cached_features_file)\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n",
    "    all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n",
    "\n",
    "    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                            all_example_index, all_cls_index, all_p_mask)\n",
    "    return dataset, examples, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 384\n",
    "model_name = \"bert-base-cased\"\n",
    "do_lower_case = False\n",
    "dev_file = \"dev-v1.1.json\"\n",
    "doc_stride = 128\n",
    "max_query_length = 64\n",
    "null_score_diff_threshold = 0\n",
    "max_answer_length = 30\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=do_lower_case)\n",
    "output_null_log_odds_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at %s dev-v1.1.json\n",
      "Saving features into cached file %s Ecached_dev_bert-base-cased_384\n"
     ]
    }
   ],
   "source": [
    "dataset, examples, features = load_and_cache_examples(dev_file, tokenizer)\n",
    "eval_sampler = SequentialSampler(dataset)\n",
    "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/srl-model-2018.05.25.tar.gz\")\n",
    "predictor._model = predictor._model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10881/10881 [36:03<00:00,  5.09it/s]\n"
     ]
    }
   ],
   "source": [
    "srls = {}\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    example_indices = batch[3]\n",
    "    for example_idx in example_indices:\n",
    "        eval_feature = features[example_idx.item()]\n",
    "        unique_id = int(eval_feature.unique_id)\n",
    "        srls[eval_feature.example_index] = predictor.predict(sentence=examples[eval_feature.example_index].question_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10570"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10881"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which NFL team represented the AFC at Super Bowl 50?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0].question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10570"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(srls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
